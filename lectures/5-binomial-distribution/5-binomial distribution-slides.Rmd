---
title: "Binomial Probability Distribution"
author: "Eric Scott"
date: "2020-01-28"
output:
  powerpoint_presentation:
    reference_doc: "template.pptx"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
source(here("plot_theme.R"))
```

## Questions from last time:

### Why does the similarity between Likelihood and P(model|data) break down with informative priors?
- For likelihood, *k* is the same for all models, as long as fit to the same data

$$
L(model_1|data) = P(data|model_1)\times k
$$
$$
L(model_2|data) = P(data|model_2)\times k
$$

- Bayes' Theorum resembles likelihood when priors are uniform because P(data) and P(model) are also the same for all models.  P(model)/P(data) -> *k*

$$
P(model_1|data) = \frac{P(data|model_1)P(model_1)}{P(data)}
$$
But when P(model1) ≠ P(model2), then P(model)/P(data) is no longer the same for all models

## Questions from last time:

### Where does P(data) come from?

- reproduce figure from [https://www.probabilisticworld.com/anatomy-bayes-theorem/]

## Probabilty distributions

- A probability **distribution** is the probability assigned to each possible value of the random variable (outcome of an experiment or observation)

- So far, we've been dealing with only two possible mutually exclusive outcomes.

- This is called a Bernouli probability, and distribution of probabilities is only defined by P("success").

```{r echo=FALSE}
df <- tibble(x = c("lives", "dies"), y = c(0.8, 1-0.8))
ggplot(df, aes(x = x, y = y)) +
  geom_col() +
  ylim(0,1) +
  labs(x = "", y = "P(n)", title = "p(success) = 0.8")
```

We write the distribution as:

$$
X \sim Bernouli(p)
$$

Where *X* is the random variable (outcome) and *p* is probability of success and "~" is read as "distributed as".


## Binomial distribution

What if we have more than one trial?

The Binomial distribution is related to Bernoulli.  It shows the probability of getting *k* events out of *N* unordered trials, if each trial has probability *p* of an event. 
by convention	

- *N* = # trials
- *k* = # events (AKA "successes")
- *p* = probability an event occurs 


## Binomial example

Example: All possible outcomes from 2 trials (with independent events), and probability p:
	e.g., say you do **two** walks and see one butterfly [N = 2 trials (a walk), k = 1 event (a butterfly)].


| trial 1| trial 2|
|--:|--:|
|       1|	     1|	
|       1|       0|	
|       0|       1|	
|       0|       0|	

So there’s only **ONE** possibility where you see two butterflies, only **ONE** possibility that you see no butterflies, but **TWO** ways to see one butterfly. 

Thus:

- Pr{k=2|N=2, p} = p^2^ (walk 1 AND walk 2)
- Pr{k=0|N=2, p} = (1-p)^2^ (not walk 1 AND not walk 2)
- Pr{k=1|N=2, p} = p(1-p) + (1-p)p = 2p(1-p)	(walk 1 AND not walk 2 OR walk 2 AND not walk 1)

(Axiom #1 – mutually exclusive events)

## It's a distribution!

This is a “distribution” because it shows the probabilities of different outcomes, given the data

- x-axis = # of events
- y-axis = probability

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(glue) #lets you put variables into strings with {}.  Alternative to paste()

df1 <- tibble(N = "N = 20",
              p = "p = 0.1",
              x = 0:20,
              y = dbinom(x, size = 20, prob = 0.1))

df2 <- tibble(N = "N = 20",
              p = "p = 0.5",
              x = 0:20,
              y = dbinom(x, size = 20, prob = 0.5))

df3 <- tibble(N = "N = 40",
              p = "p = 0.1",
              x = 0:40,
              y = dbinom(x, size = 40, prob = 0.1))

df4 <- tibble(N = "N = 40",
              p = "p = 0.5",
              x = 0:40,
              y = dbinom(x, size = 40, prob = 0.5))

bind_rows(df1, df2, df3, df4) %>% 
ggplot(aes(x,y)) +
  geom_col() +
  labs(x = "# events",
       y = "probability") +
  facet_grid(p~N, scales = "free_x")
```


## Binomial distribution as an equation:

$$
Pr \lbrace x = k|N,p \rbrace = \big(p^k(1-p)^{N-k} \big) {n \choose x}
$$
Walking through this equation:

- p^k^ = probability of recapturing k particular events (Axiom # 2 – independent events)

- (1-p)^N-k^  = the probability of not recapturing N-k particular particular (Axiom #2)

- ${n \choose k}$ = the number of ways you can get k events in N trials 


**Show how this simplifies in our example with two walks.**

## N choose k

to understand the meaning of “the number of ways you can get k events in N trials”, write out all possibilities.

${n \choose k}$ is mathematical notation for the number of ways you can get k events in N trials:

$$
{n \choose k} = \frac{N!}{k!(N-k)!} = \frac{N \times (N-1) \times (N-2) \times ...\times [N-(N-1)]}{\lbrace k \times (k-1) \times ... \times [k-(k-1)]\rbrace \lbrace(N-k) \times (N-k-1)\times ... \times [N-k-(N-k-1)]}
$$
e.g.,  

$$
{2 \choose 1} = \frac{2!}{2!(2-1)!} = \frac{2 \times 1}{1 \times 1} = 2
$$

and  

$$
{2 \choose 2} = \frac{2!}{2!(0!)} = \frac{2 \times 1}{2\times 1 \times 1} = 1
$$

(NOTE: By definition, 0! = 1)



## Using Binomial distribution to calculate support for a model:  

- Recall that L(model|data) is proportional to P(data|model).
- use the Binomial distribution to calculate the likelihood of values of p, given N = 2, k = 1

Possible parameters:

$$
L(k = 1 | N = 2, p) = (p^k(1-p)^{N-k}){N \choose k}
$$

$$
= (p^1(1-p)^{2-1}){2 \choose 1} = 2p(1-p)
$$
Can substitute various values of p to see which are the most likely to give us one event out of 2 trials.

## Binomial distribution in R

In R, you can get the probability of data given a binomial model using the function `dbinom()`

```
dbinom(x, size, prob, log)
```
where

- `x` = # events, k
- `size` = # trails, N
- `prob` = probability of an event in a single trial, p
- `log = TRUE` or `FALSE` … do you want the log of the probability, or the probability?

## Use R to calculate likelihood

Because probability of the data (output from `dbinom()`) is proportional to likelihood, we can calculate a likelihood *profile* in R.

```{r echo=FALSE}
df <- tibble(p = seq(0.01, 0.99, length.out = 20),
             loglik = dbinom(x = 1, size = 2, prob = p, log = TRUE))

ggplot(df, aes(x = p, y = loglik)) +
  geom_line() +
  geom_hline(aes(yintercept = max(loglik)), color = "red") +
  labs(x = "value of p", y = "log-likelihood", title = "Likelihood profile for binomial with N = 2, k = 1")
```

The **maximum likelihood estimate** is the value where the slope of the line is zero: its the parameter values that maximize likelihood of the data, given the parameters.  Here, it's p = 0.5 for the probability of getting 1 success in 2 trials.

## True Facts about binomial distribution

- the average (or expected) number of events, $k$, for $N$ trials and probabilty $p$ is $Np$
- $\mu = Np$
- The variance among the number of events of multiple sets of $N$ trials is $Np(1-p)$.
- $\sigma^2 = Np(1-p)$
- Standard deviation = $\sigma = \sqrt{Np(1-p)}$
- The maximum likelihood estimate (MLE) of $p$ for a dataset with $N$ trials and $k$ events is $k/N$

```{r echo=FALSE}
library(latex2exp)
p_success <- 0.3
N <- 30

mu = N * p_success
var = N * p_success * (1-p_success)
sd = sqrt(var)

plot_data1 <- tibble(x = 0:N,
                     y = dbinom(x, size = N, prob = p_success))
ggplot(plot_data1, aes(x,y)) +
  geom_col(color = "#449F8E", fill = "#449F8E") +
  geom_vline(aes(xintercept = mu), color = "red") +
  geom_segment(aes(y = mean(y), yend = mean(y), x = mu - sd, xend = mu + sd), color = "red") +
  labs(title = glue("Binomial distribution for N = {N} trials, p = {p_success}"),
       x = "# events",
       y = "probability",
       subtitle = TeX(glue("$\\mu = {mu}$; $\\sigma ^2 = {var}$; $\\sigma = {round(sd, 2)}$")))
```
```{r echo=FALSE}
library(latex2exp)
p_success <- 0.7
N <- 15

mu = N * p_success
var = N * p_success * (1-p_success)
sd = sqrt(var)

plot_data1 <- tibble(x = 0:N,
                     y = dbinom(x, size = N, prob = p_success))
ggplot(plot_data1, aes(x,y)) +
  geom_col(color = "#449F8E", fill = "#449F8E") +
  geom_vline(aes(xintercept = mu), color = "red") +
  geom_segment(aes(y = mean(y), yend = mean(y), x = mu - sd, xend = mu + sd), color = "red") +
  labs(title = glue("Binomial distribution for N = {N} trials, p = {p_success}"),
       x = "# events",
       y = "probability",
       subtitle = TeX(glue("$\\mu = {mu}$; $\\sigma ^2 = {var}$; $\\sigma = {round(sd, 2)}$")))
```

## Ecological significance

- For binomial processes (e.g. survival), the variance in the number of events is a function of the number of trials and probability (p).  Variance is **not** a free parameter (the way it is in the Normal distribution).
- If a process is probabilistic, we expect variation in the outcome of some number of trials, N (even if we take very precise measurements, and know p very precisely, we cannot predict the outcome of any particular set of trials)
- You can use the “null” binomial variance as a basis for estimating other sources of variation, such as differences among years, sites, or individual animals or plants

# Worked example:  Anne’s calypso orchids…

## Load a few convenience packages

```{r include=FALSE}
library(here) #reproducible file paths
library(readr) #'safer' version of read.csv()
```

## Read in data

```{r include=FALSE}
orchids <- read_csv(here("data", "orchid.seedlings.csv"))
head(orchids)
```

We want to know the number of seedlings in year 0 (`seedlings.0`) as a function of seeds in year -1, -2, -3, or -4.

- Seedlings in year 0 is our `x` (*k*), or number of events.  
- Our size, *N*, is how many seeds we think we started out with, depending on the lag (of 1, 2, 3, or 4 years). 
- Calculate the probability of the data with binomial distribution (given a model). 

Here our "model" has two parts: 

1. our hypothesis about the lag, i.e., how many years (influences N)
2. our binomial probability of a seed resulting in a seedling (p)

## Take the case where the lag is 4, and prob = 0.05

`dbinom(x, size, prob, log=F)`

```{r}
lnprob_m1 = dbinom(x = orchids$seedlings.0, size = orchids$seeds.m4, prob = 0.05, log = TRUE)
lnprob_m1 # show the list of probabilities for each observation, given the model
sum(lnprob_m1) # log-likelihood of the whole data set
```


## Compare to a lag of 4 and seeds per seedling of 0.04

```{r}
lnprob_m2 = dbinom(x = orchids$seedlings.0, size = orchids$seeds.m4, prob = 0.04, log = TRUE)
lnprob_m2 # show the list of probabilities of each observation, given the model
sum(lnprob_m2) # log-likelihood of the whole data set
```

## Compare to a lag of 3 and seeds per seedling of 0.05

```{r}
lnprob_m3 = dbinom(x = orchids$seedlings.0, size = orchids$seeds.m3, prob = 0.05, log = TRUE)
lnprob_m3 # show the list of probabilities of each observation, given the model
sum(lnprob_m3) # log-likelihood of the whole data set
```

We could do this for the entire parameter space of binomial probabilities and lags, but this is inefficient.  Next lab you will learn to write for loops that can do this kind of thing with less code.


