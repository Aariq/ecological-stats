---
title: "Bayes' Theorem Continued"
author: "Eric Scott"
date: "2020-1-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(tidyverse)
# source(here::here("plot_theme.R"))
library(patchwork)
```

## Bayes' Theorem Continued

$$
P(B│A)=  \frac{(P(A│B)P(B))}{(P(A))} \Rightarrow P(model│data) = \frac{(P(data│model)P(model))}{(P(data))}
$$

Back to the perennial flower example...

We have already calculated P(data|model) for our simple plant demography example

What is P(model), the "prior probability"?

- the probability of each possible model *prior to* collection or analysis of the data being analyzed.  In many cases, we want to base inference **only** on our data, in whcih case we use **uninformative priors**
- for *j* models, each P(model) is 1/*J*
- This differes from our Lyme disease example, where we had **informative priors** that came from information about the prevalence of Lyme disease (known before we did the test and collected our data)

What is P(data)?

Assume all possible and mutually exclusive models are considered in the set of *j* models.  For each model, calculate P(data|model)*P(model) and sum over all j models. (OR rule)

$$
P(data) = \sum_{j=1}^nP(data|model_j)\times P(model_j)
$$

When priors are uninformative, P(model)  = 1/*j* and Bayes' Theorem begins to resemble likelihood

$$
P(model|data) = P(data|model)\times \frac{(1/j)}{c}
$$

## Apply to flower data with uninformative prior

```{r}
s <- seq(0.1, 0.9, 0.1)
s
orchid_lg_lik <- s^8 * (1-s)^2

df_un <- tibble(`survival probability` = s,
                   `P(data|model)` = orchid_lg_lik) %>% 
  mutate(`P(model)` = 1/n())

df_un
```
```{r}
p_data = sum(df_un$`P(data|model)` * df_un$`P(model)`)
p_data
```
```{r}
df_un <- df_un %>%
  mutate(numerator = `P(data|model)` * `P(model)`) %>% 
  mutate(`P(model|data)` = `P(data|model)` * `P(model)` / sum(numerator))
df_un
```

```{r}
lik_un <- ggplot(df_un, aes(x = `survival probability`, y = `P(data|model)`)) +
  geom_point(color = "orange") +
  geom_line(color = "orange") +
  
  labs(title = "Probability of data given model")
```

```{r}
prior_un <- ggplot(df_un, aes(x = `survival probability`, y = `P(model)`)) +
    geom_point(color = "blue") +
  geom_line(color = "blue") +
  
  labs(title = "uninformative prior")
```

```{r}
prob_un <- ggplot(df_un, aes(x = `survival probability`, y = `P(model|data)`)) +
  geom_point(color = "green") +
  geom_line(color = "green") +
  
  labs(title = "probability of model given data")
```


```{r}
lik_un
prior_un
prob_un
```

## Bayes' theorem with informative prior

```{r}
df_un
```

## Protocorm problem

```{r}
p_model1 = 1.3*10^-9
p_others = (1-p_model1)/3
p_model <- log(c(p_model1, rep(p_others, 3)))

p_data_model <- c(-67.5, -121.1, -124.5, -86.8)

tibble(p_model, p_data_model) %>% 
  mutate(p_model_data = p_data_model + p_model + sum(p_data_model + p_model))
```

Now model 4 has a lower log-likelihood

```{r}
exp(-67.5)
sum(
  exp(-121.1), exp(-124.5), exp(-86.8)
)
```

